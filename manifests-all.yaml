# Derived from ./manifests
---
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: elasticsearch
  labels:
    app: elasticsearch
    stack: logging
rules:
- apiGroups:
  - extensions
  resources:
  - podsecuritypolicies
  resourceNames:
  - elasticsearch
  verbs:
  - use
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: elasticsearch
  labels:
    app: elasticsearch
    stack: logging
subjects:
- kind: ServiceAccount
  name: elasticsearch
  namespace: default
roleRef:
  kind: ClusterRole
  name: elasticsearch
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: elasticsearch
  labels:
    app: elasticsearch
    stack: logging
---
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: elasticsearch
  labels:
    app: elasticsearch
    stack: logging
spec:
  fsGroup:
    rule: RunAsAny
  privileged: true
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  allowedCapabilities:
  - 'IPC_LOCK'
  - 'SYS_RESOURCE'
  supplementalGroups:
    rule: RunAsAny
  volumes:
  - '*'
  hostPID: true
  hostIPC: true
  hostNetwork: true
  hostPorts:
  - min: 1
    max: 65536
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: elasticsearch
  labels:
    app: elasticsearch
    stack: logging
data:
  elasticsearch.yml: |
    cluster.name: full-stack-cluster
    node.name: node-1
    path.data: /usr/share/elasticsearch/data
    http:
      host: 0.0.0.0
      port: 9200
    bootstrap.memory_lock: true
    transport.host: 127.0.0.1
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: elasticsearch
  labels:
    app: elasticsearch
    stack: logging
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: elasticsearch
        stack: logging
    spec:
      serviceAccountName: elasticsearch
      initContainers:
      - name: set-vm-max-map-count
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ['sysctl', '-w', 'vm.max_map_count=262144']
        securityContext:
          privileged: true
      - name: volume-mount-hack
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c", "chown -R 1000:100 /usr/share/elasticsearch/data"]
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.0.0
        imagePullPolicy: IfNotPresent
        env:
        - name: ES_JAVA_OPTS
          value: -Xms1024m -Xmx1024m
          # ES_MEM_LIMIT=2g
          # ES_JVM_HEAP=1024m
        ports:
        - containerPort: 9200
        resources:
          limits:
            memory: "2147483648"
        volumeMounts:
        - name: config
          mountPath: /usr/share/elasticsearch/elasticsearch.yml
          subPath: elasticsearch.yml
        - name: data
          mountPath: /usr/share/elasticsearch/data
      # Allow non-root user to access PersistentVolume
      securityContext:
        fsGroup: 1000
      restartPolicy: Always
      volumes:
      - name: config
        configMap:
          name: elasticsearch
      - name: data
        persistentVolumeClaim:
          claimName: elasticsearch
      # - name: data
      #   hostPath:
      #     path: /srv/elasticsearch-data
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: elasticsearch
  labels:
    app: elasticsearch
    stack: logging
spec:
  rules:
  -
    # host: elasticsearch.minikube.localnet
    host: elasticsearch.j9egj.k8s.ginger.eu-central-1.aws.gigantic.io
    http:
      paths:
      - path: /
        backend:
          serviceName: elasticsearch
          servicePort: 9200
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: elasticsearch
  labels:
    app: elasticsearch
    stack: logging
spec:
  # storageClassName: standard
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1G
---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  labels:
    app: elasticsearch
    stack: logging
spec:
  # needed on minikube
  type: NodePort
  ports:
  - name: "api"
    port: 9200
    targetPort: 9200
  selector:
    app: elasticsearch
    stack: logging
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fluentd
  namespace: default
  labels:
    app: fluentd
    stack: logging
rules:
- apiGroups:
  - extensions
  resources:
  - podsecuritypolicies
  resourceNames:
  - fluentd
  verbs:
  - use
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fluentd
  namespace: default
  labels:
    app: fluentd
    stack: logging
subjects:
- kind: ServiceAccount
  name: fluentd
roleRef:
  kind: Role
  name: fluentd
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  labels:
    app: fluentd
    stack: logging
---
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: fluentd
  labels:
    app: fluentd
    stack: logging
spec:
  fsGroup:
    rule: RunAsAny
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  volumes:
  - emptyDir
  - secret
  - downwardAPI
  - configMap
  - persistentVolumeClaim
  - projected
  - hostPath
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fluentd
  labels:
    app: fluentd
    stack: logging
rules:
- apiGroups: [""] # core API group
  resources: ["pods", "namespaces"]
  verbs: ["get", "watch", "list"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fluentd
  labels:
    app: fluentd
    stack: logging
subjects:
- kind: ServiceAccount
  name: fluentd
  namespace: default
roleRef:
  kind: ClusterRole
  name: fluentd
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd
  # namespace: logging-fluentd
  labels:
    app: fluentd
data:
  # see also https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/fluentd-es-image/td-agent.conf
  # https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter
  # https://groups.google.com/forum/#!msg/fluentd/CMZf4cTPlow/GADnOFIsBQAJ
  # https://github.com/fabric8io/docker-fluentd-kubernetes/issues/11
  # > Yeah, I use this plugin to regroup multilines (with multiline_start_regexp parameter) from my JSON docker logs on kubernetes (with ES /Fluentd / Kibana associated ), an then I parse them with fluent-plugin-parser , so I can handle correctly traceback and other multilines logs in elasticsearch.
  #
  # fluent-plugin-concat, fluent-plugin-parser, rewrite_tag_filter
  # https://github.com/fluent/fluent-plugin-rewrite-tag-filter
  #
  # http://dev.haufe.io/fluentd-log-parsing/ !!
  #
  # fluent-plugin-grok-parser
  #
  # https://github.com/fluent/fluentd/blob/master/ChangeLog

  fluent.conf: |
    @include kubernetes.conf

    <match **>
      type elasticsearch
      log_level info
      include_tag_key true
      host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
      port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
      scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
      # user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
      # password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
      reload_connections "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS'] || 'true'}"
      logstash_prefix "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX'] || 'logstash'}"
      logstash_format true
      buffer_chunk_limit 2M
      buffer_queue_limit 32
      flush_interval 5s
      max_retry_wait 30
      disable_retry_limit
      num_threads 8
    </match>

  kubernetes.conf: |
    # https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter/blob/master/README.md

    <match fluent.**>
      type null
    </match>

    <source>
      type tail
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      time_format %Y-%m-%dT%H:%M:%S.%NZ
      # tag_to_kubernetes_name_regexp \.(?<pod_name>[^\._]+)_(?<namespace>[^_]+)_(?<container_name>.+)-(?<docker_id>[a-z0-9]{64})\.log$</pod>)
      tag kubernetes.*
      format json
      read_from_head true
    </source>

    <source>
      type tail
      format /^(?<time>[^ ]* [^ ,]*)[^\[]*\[[^\]]*\]\[(?<severity>[^ \]]*) *\] (?<message>.*)$/
      time_format %Y-%m-%d %H:%M:%S
      path /var/log/salt/minion
      pos_file /var/log/fluentd-salt.pos
      tag salt
    </source>

    <source>
      type tail
      format syslog
      path /var/log/startupscript.log
      pos_file /var/log/fluentd-startupscript.log.pos
      tag startupscript
    </source>

    <source>
      type tail
      format /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
      path /var/log/docker.log
      pos_file /var/log/fluentd-docker.log.pos
      tag docker
    </source>

    <source>
      type tail
      format none
      path /var/log/etcd.log
      pos_file /var/log/fluentd-etcd.log.pos
      tag etcd
    </source>

    <source>
      type tail
      format kubernetes
      multiline_flush_interval 5s
      path /var/log/kubelet.log
      pos_file /var/log/fluentd-kubelet.log.pos
      tag kubelet
    </source>

    <source>
      type tail
      format kubernetes
      multiline_flush_interval 5s
      path /var/log/kube-proxy.log
      pos_file /var/log/fluentd-kube-proxy.log.pos
      tag kube-proxy
    </source>

    <source>
      type tail
      format kubernetes
      multiline_flush_interval 5s
      path /var/log/kube-apiserver.log
      pos_file /var/log/fluentd-kube-apiserver.log.pos
      tag kube-apiserver
    </source>

    <source>
      type tail
      format kubernetes
      multiline_flush_interval 5s
      path /var/log/kube-controller-manager.log
      pos_file /var/log/fluentd-kube-controller-manager.log.pos
      tag kube-controller-manager
    </source>

    <source>
      type tail
      format kubernetes
      multiline_flush_interval 5s
      path /var/log/kube-scheduler.log
      pos_file /var/log/fluentd-kube-scheduler.log.pos
      tag kube-scheduler
    </source>

    <source>
      type tail
      format kubernetes
      multiline_flush_interval 5s
      path /var/log/rescheduler.log
      pos_file /var/log/fluentd-rescheduler.log.pos
      tag rescheduler
    </source>

    <source>
      type tail
      format kubernetes
      multiline_flush_interval 5s
      path /var/log/glbc.log
      pos_file /var/log/fluentd-glbc.log.pos
      tag glbc
    </source>

    <source>
      type tail
      format kubernetes
      multiline_flush_interval 5s
      path /var/log/cluster-autoscaler.log
      pos_file /var/log/fluentd-cluster-autoscaler.log.pos
      tag cluster-autoscaler
    </source>

    <filter kubernetes.**>
      type kubernetes_metadata
    </filter>

    <filter kubernetes.var.log.containers.nginx-ingress-controller-**>
      # https://github.com/kubernetes/ingress/tree/master/controllers/nginx#log-format
      type parser
      reserve_data true
      hash_value_field parsed
      key_name log
      format /^(?<remote_addr>[^ ]*) - \[(?<proxy_add_x_forwarded_for>[^\]]*)\] - (?<remote_user>[^ ]*) \[(?<time_local>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^\"]*) +\S*)?" (?<status>[^ ]*) (?<body_bytes_sent>[^ ]*)(?: "(?<http_referer>[^\"]*)" "(?<http_user_agent>[^\"]*)")?.*$/
    </filter>

    # <filter kubernetes.var.log.containers.nginx-ingress-controller-**>
    #   type record_transformer
    #   <record>
    #     message "#{parsed.method} #{parsed.path} #{parsed.status}"
    #   </record>
    # </filter>

  # prometheus.conf: |
  #   # https://github.com/kazegusuri/fluent-plugin-prometheus#usage
  #
  #   <source>
  #     @type prometheus
  #   </source>
  #
  #   <source>
  #     @type prometheus_monitor
  #   </source>
  #
  #   <filter **>
  #     @type prometheus
  #     <metric>
  #       name fluentd_records_total
  #       type counter
  #       desc The total number of records read by fluentd.
  #     </metric>
  #   </filter>
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: fluentd
  labels:
    app: fluentd
spec:
  template:
    metadata:
      name: fluentd
      labels:
        app: fluentd
    spec:
      serviceAccountName: fluentd
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v0.12.33-elasticsearch
        imagePullPolicy: IfNotPresent
        env:
        - name:  FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch.default.svc"
        - name:  FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        - name:  FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX
          value: "fluentd"
        - name: DEBUG
          value: "true"
        volumeMounts:
        - name: config
          mountPath: /fluentd/etc
        - name: host-var-log
          mountPath: /var/log
        - name: host-var-lib-docker-containers
          mountPath: /var/lib/docker/containers
          # readOnly: true
        - name: minikube-var-lib-docker-containers
          mountPath: /mnt/sda1/var/lib/docker/containers
        # securityContext:
        #   runAsUser: 0
      volumes:
      - name: config
        configMap:
          name: fluentd
      - name: host-var-log
        hostPath:
          path: /var/log
      - name: host-var-lib-docker-containers
        hostPath:
          path: /var/lib/docker/containers
      - name: minikube-var-lib-docker-containers
        hostPath:
          path: /mnt/sda1/var/lib/docker/containers
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kibana
  labels:
    app: kibana
    stack: logging
data:
  kibana.yml: |
    server:
      name: "full-stack-example"
      port: 127.0.0.1:5601
    elasticsearch.url: "http://elasticsearch:9200"
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: kibana
  labels:
    app: kibana
    stack: logging
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: kibana
        stack: logging
    spec:
      # FIXME
      # healthcheck + resources
      containers:
      - name: kibana
        image: docker.elastic.co/kibana/kibana-oss:6.0.0
        imagePullPolicy: IfNotPresent
        env:
        - name: ELASTICSEARCH_PASSWORD
          value: changeme
        ports:
        - containerPort: 5601
        resources: {}
        volumeMounts:
        - name: config
          mountPath: /usr/share/kibana/kibana.yml
          subPath: kibana.yml
      restartPolicy: Always
      volumes:
      - name: config
        configMap:
          name: kibana
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kibana
  labels:
    app: kibana
    stack: logging
  annotations:
    ingress.kubernetes.io/auth-signin: https://$host/oauth2/start
    ingress.kubernetes.io/auth-url: https://$host/oauth2/auth
    kubernetes.io/tls-acme: "true"
spec:
  rules:
  - host: kibana.codeformuenster.org
    http:
      paths:
      - path: /
        backend:
          serviceName: kibana
          servicePort: 5601
  tls:
  - secretName: kibana-tls
    hosts:
    - kibana.codeformuenster.org

---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kibana-auth
  labels:
    app: kibana-auth
    stack: logging
  annotations:
    kubernetes.io/tls-acme: "true"
spec:
  rules:
  - host: kibana.codeformuenster.org
    http:
      paths:
      - path: /oauth2
        backend:
          serviceName: oauth2-proxy
          servicePort: 4180
  tls:
  - secretName: kibana-tls
    hosts:
    - kibana.codeformuenster.org
---
apiVersion: v1
kind: Service
metadata:
  name: kibana
  labels:
    app: kibana
    stack: logging
spec:
  type: NodePort
  ports:
  - name: "ui"
    port: 5601
    targetPort: 5601
  selector:
    app: kibana
    stack: logging
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: oauth2-proxy
  labels:
    app: oauth2-proxy
data:
  oauth2_proxy.cfg: |
    # for reference see https://github.com/bitly/oauth2_proxy/blob/master/contrib/oauth2_proxy.cfg.example

    http_address = "0.0.0.0:4180"
    upstreams = ["file:///dev/null"]

    provider = "github"
    email_domains = ["*"]
    github_org = "codeformuenster"
    github_team = "kube-admin,hacker"
    client_id = "16cf63d2612dc04c938f"
    client_secret = "e1ef428865fa6f05c6256d20a7bccdd027392fc2"
    cookie_secret = "vJsbnX5uFU0KY5OYa6a6gA=="
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: oauth2-proxy
  labels:
    app: oauth2-proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: oauth2-proxy
  template:
    metadata:
      labels:
        app: oauth2-proxy
    spec:
      containers:
      - name: oauth2-proxy
        image: giantswarm/oauth2_proxy:master
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 4180
          protocol: TCP
        volumeMounts:
        - name: config
          mountPath: /etc/oauth2_proxy
      volumes:
      - name: config
        configMap:
          name: oauth2-proxy
---
apiVersion: v1
kind: Service
metadata:
  name: oauth2-proxy
  labels:
    app: oauth2-proxy
spec:
  ports:
  - name: http
    port: 4180
    protocol: TCP
    targetPort: 4180
  selector:
    app: oauth2-proxy
